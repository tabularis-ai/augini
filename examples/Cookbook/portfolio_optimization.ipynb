{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install yfinance\n",
    "%pip install augini\n",
    "%pip install PyPortfolioOpt\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onlyartist9/Projects/Tabularis/augini/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from pypfopt import EfficientFrontier, expected_returns, risk_models\n",
    "from augini import Augini\n",
    "import os\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using API key from environment variable.\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Input your api key\n",
    "def get_api_key():\n",
    "    api_key = os.environ.get('OPENROUTER_TOKEN')\n",
    "    if api_key:\n",
    "        print(\"Using API key from environment variable.\")\n",
    "        return api_key\n",
    "    else:\n",
    "        api_key = input(\"Enter your API key manually: \")\n",
    "        return api_key\n",
    "\n",
    "# Set up Augini\n",
    "api_key = get_api_key()\n",
    "augini = Augini(api_key=api_key, use_openrouter=True, model='openai/gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stock price data sample:\n",
      "                                 META        AAPL        AMZN        NFLX  \\\n",
      "Date                                                                        \n",
      "2023-12-27 00:00:00-05:00  356.468292  192.208359  153.339996  491.790009   \n",
      "2023-12-28 00:00:00-05:00  356.956421  192.636276  153.380005  490.510010   \n",
      "2023-12-29 00:00:00-05:00  352.613037  191.591385  151.940002  486.880005   \n",
      "2024-01-02 00:00:00-05:00  344.972229  184.734985  149.929993  468.500000   \n",
      "2024-01-03 00:00:00-05:00  343.159149  183.351761  148.470001  470.260010   \n",
      "\n",
      "                                GOOGL  \n",
      "Date                                   \n",
      "2023-12-27 00:00:00-05:00  139.862976  \n",
      "2023-12-28 00:00:00-05:00  139.723480  \n",
      "2023-12-29 00:00:00-05:00  139.185440  \n",
      "2024-01-02 00:00:00-05:00  137.670929  \n",
      "2024-01-03 00:00:00-05:00  138.418228  \n",
      "\n",
      "Basic statistics for each stock:\n",
      "             META        AAPL        AMZN        NFLX       GOOGL\n",
      "count  253.000000  253.000000  253.000000  253.000000  253.000000\n",
      "mean   505.812602  206.248792  183.966127  667.542529  162.918038\n",
      "std     64.074596   25.408704   17.425389  109.859830   15.455629\n",
      "min    343.159149  164.405121  144.570007  468.500000  130.925369\n",
      "25%    473.182434  183.486069  174.990005  607.150024  150.547455\n",
      "50%    503.273560  212.829224  183.149994  647.500000  163.882187\n",
      "75%    560.897522  226.960007  189.080002  706.130005  174.357956\n",
      "max    632.170044  259.019989  232.929993  936.559998  196.660004\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initial Data Collection\n",
    "\n",
    "tickers = ['META', 'AAPL', 'AMZN', 'NFLX', 'GOOGL']\n",
    "\n",
    "def get_stock_data(tickers, period='1y'):\n",
    "    data = pd.DataFrame()\n",
    "    for ticker in tickers:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        hist = stock.history(period=period)\n",
    "        data[ticker] = hist['Close']\n",
    "    return data\n",
    "\n",
    "# Get stock price data\n",
    "stock_data = get_stock_data(tickers)\n",
    "print(\"\\nStock price data sample:\")\n",
    "print(stock_data.head())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic statistics for each stock:\")\n",
    "print(stock_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recent news data summary:\n",
      "Total news articles retrieved: 50\n",
      "\n",
      "Articles per ticker:\n",
      "ticker\n",
      "AAPL     10\n",
      "GOOGL    10\n",
      "NFLX     10\n",
      "AMZN     10\n",
      "META     10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample of recent news:\n",
      "   ticker                                              title  \\\n",
      "16   AAPL  Dow Jones Slides 280 Points As Treasury Yields...   \n",
      "17   AAPL  Made In China, Paid In America: What Trump's N...   \n",
      "48  GOOGL  OpenAI Wants to Create a â€˜Public Benefit Corpo...   \n",
      "30   NFLX  Netflix sets NFL streaming record with Christm...   \n",
      "35   NFLX  Shareholders Donâ€™t Vote as If Their Power Matt...   \n",
      "\n",
      "                  date  \n",
      "16 2024-12-27 14:32:50  \n",
      "17 2024-12-27 13:58:42  \n",
      "48 2024-12-27 13:57:00  \n",
      "30 2024-12-27 13:52:32  \n",
      "35 2024-12-27 13:30:00  \n",
      "\n",
      "Missing values in each column:\n",
      "ticker         0\n",
      "title          0\n",
      "description    0\n",
      "link           0\n",
      "publisher      0\n",
      "date           0\n",
      "dtype: int64\n",
      "\n",
      "News date range:\n",
      "Earliest: 2024-12-26 09:03:32\n",
      "Latest: 2024-12-27 14:32:50\n"
     ]
    }
   ],
   "source": [
    "# Step 2: News Data Collection using yfinance Search\n",
    "\n",
    "def get_stock_news(tickers, news_count=10):\n",
    "    news_data = []\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Use Search class to get news\n",
    "            search_results = yf.Search(\n",
    "                query=ticker,\n",
    "                news_count=news_count,\n",
    "                max_results=news_count\n",
    "            )\n",
    "            \n",
    "            if hasattr(search_results, 'news') and search_results.news:\n",
    "                for news_item in search_results.news:\n",
    "                    try:\n",
    "                        news_data.append({\n",
    "                            'ticker': ticker,\n",
    "                            'title': news_item.get('title', ''),\n",
    "                            'description': news_item.get('description', ''),\n",
    "                            'link': news_item.get('link', ''),\n",
    "                            'publisher': news_item.get('publisher', ''),\n",
    "                            'date': pd.to_datetime(news_item.get('providerPublishTime', pd.Timestamp.now().timestamp()), unit='s')\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing news item for {ticker}: {str(e)}\")\n",
    "                        continue\n",
    "            else:\n",
    "                print(f\"No news found for {ticker}\")\n",
    "                news_data.append({\n",
    "                    'ticker': ticker,\n",
    "                    'title': 'No recent news available',\n",
    "                    'description': 'No description available',\n",
    "                    'link': '',\n",
    "                    'publisher': '',\n",
    "                    'date': pd.Timestamp.now()\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching news for {ticker}: {str(e)}\")\n",
    "            news_data.append({\n",
    "                'ticker': ticker,\n",
    "                'title': 'Failed to fetch news',\n",
    "                'description': 'Error in data retrieval',\n",
    "                'link': '',\n",
    "                'publisher': '',\n",
    "                'date': pd.Timestamp.now()\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    \n",
    "    # Sort by date (most recent first)\n",
    "    news_df = news_df.sort_values('date', ascending=False)\n",
    "    \n",
    "    return news_df\n",
    "\n",
    "# Fetch news data\n",
    "news_df = get_stock_news(tickers)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nRecent news data summary:\")\n",
    "print(f\"Total news articles retrieved: {len(news_df)}\")\n",
    "print(\"\\nArticles per ticker:\")\n",
    "print(news_df['ticker'].value_counts())\n",
    "\n",
    "print(\"\\nSample of recent news:\")\n",
    "# Display sample showing ticker, title, and date\n",
    "print(news_df[['ticker', 'title', 'date']].head())\n",
    "\n",
    "# Quality checks\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(news_df.isnull().sum())\n",
    "\n",
    "# Date range of news\n",
    "print(\"\\nNews date range:\")\n",
    "print(f\"Earliest: {news_df['date'].min()}\")\n",
    "print(f\"Latest: {news_df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original vs Augmented Data Sample:\n",
      "   ticker                                              title sentiment\n",
      "16   AAPL  Dow Jones Slides 280 Points As Treasury Yields...   Bearish\n",
      "17   AAPL  Made In China, Paid In America: What Trump's N...   Bullish\n",
      "48  GOOGL  OpenAI Wants to Create a â€˜Public Benefit Corpo...   Bearish\n",
      "30   NFLX  Netflix sets NFL streaming record with Christm...   Bearish\n",
      "35   NFLX  Shareholders Donâ€™t Vote as If Their Power Matt...   Bearish\n",
      "\n",
      "Sentiment Distribution:\n",
      "sentiment\n",
      "Bullish    20\n",
      "Bearish    16\n",
      "Neutral    14\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Synthetic Data Generation using Augini\n",
    "\n",
    "sentiment_prompt = \"\"\"\n",
    "Analyze the news title and description to determine market sentiment.\n",
    "Generate synthetic sentiment labels across three categories:\n",
    "- Bearish (negative outlook)\n",
    "- Neutral (stable outlook)\n",
    "- Bullish (positive outlook)\n",
    "Base the sentiment on market implications and investor perspective. The name of the column returned will be 'sentiment'.\n",
    "\"\"\"\n",
    "\n",
    "# Generate synthetic sentiment data\n",
    "augmented_df = augini.augment_single(\n",
    "    news_df,\n",
    "    column_name=\"sentiment\",\n",
    "    custom_prompt=sentiment_prompt,\n",
    "    use_sync=False\n",
    ")\n",
    "\n",
    "print(\"\\nOriginal vs Augmented Data Sample:\")\n",
    "print(augmented_df[['ticker', 'title', 'sentiment']].head())\n",
    "\n",
    "# Convert sentiment to numerical scores\n",
    "sentiment_scores = {\n",
    "    'Bearish': -1,\n",
    "    'Neutral': 0,\n",
    "    'Bullish': 1\n",
    "}\n",
    "\n",
    "augmented_df['sentiment_score'] = augmented_df['sentiment'].map(sentiment_scores)\n",
    "\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "print(augmented_df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Data Overview ðŸ“Š\n",
       "\n",
       "The dataset consists of **50 rows** and **8 columns**. It contains news articles associated with various stock tickers, including their titles, descriptions, publication links, publishers, publication dates, sentiment analysis, and sentiment scores.\n",
       "\n",
       "### Column Details\n",
       "- **Ticker**: Represents stock tickers (5 unique values). The most frequent tickers are AAPL, GOOGL, NFLX, AMZN, and META, each appearing 10 times.\n",
       "- **Title**: Contains titles of articles (44 unique values). The most common titles appear twice.\n",
       "- **Description**: All entries are empty, indicating a lack of description content.\n",
       "- **Link**: Contains URLs of articles (44 unique values). Several links are repeated twice.\n",
       "- **Publisher**: Lists the publishers (15 unique values). Most articles are published by 'Motley Fool', followed by 'Insider Monkey' and 'Zacks'.\n",
       "- **Date**: Indicates the publication date and time (43 unique values). The most frequent dates include several entries on '2024-12-27'.\n",
       "- **Sentiment**: Categorizes the sentiment of the articles into three groups: Bullish (20), Bearish (16), Neutral (14).\n",
       "- **Sentiment Score**: Numerical representation of sentiment with a mean score of **0.08** and a range from **-1.0 to 1.0**.\n",
       "\n",
       "### Data Quality\n",
       "- **Missing Values**: There are no missing values across all columns.\n",
       "- **Duplicated Rows**: The dataset has no duplicate entries.\n",
       "\n",
       "### Sample Data\n",
       "Here are the first three rows:\n",
       "1. **AAPL** - \"Dow Jones Slides 280 Points As Treasury Yields Climb; Nvidia, Tesla Drop\" - Sentiment: Bearish (Score: -1)\n",
       "2. **AAPL** - \"Made In China, Paid In America: What Trump's New Tariffs Mean For The US Economy\" - Sentiment: Bullish (Score: 1)\n",
       "3. **GOOGL** - \"OpenAI Wants to Create a â€˜Public Benefit Corporation.â€™ Hereâ€™s What That Means.\" - Sentiment: Bearish (Score: -1)\n",
       "\n",
       "### Limitations\n",
       "- The **description** column lacks content, limiting the depth of information provided by each article."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(augini.chat(\"Give me an overview of the data.\",df=augmented_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adjusted Expected Returns:\n",
      "META: 0.6418\n",
      "AAPL: 0.3627\n",
      "AMZN: 0.4434\n",
      "NFLX: 0.8460\n",
      "GOOGL: 0.3919\n",
      "\n",
      "Optimized Portfolio Weights:\n",
      "META: 0.1619\n",
      "AAPL: 0.2094\n",
      "AMZN: 0.0000\n",
      "NFLX: 0.6013\n",
      "GOOGL: 0.0274\n",
      "\n",
      "Portfolio Performance Metrics:\n",
      "Expected Annual Return: 69.93%\n",
      "Annual Volatility: 22.94%\n",
      "Sharpe Ratio: 2.96\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Portfolio Optimization with Sentiment Integration\n",
    "\n",
    "# Calculate base returns and risk\n",
    "returns = expected_returns.mean_historical_return(stock_data)\n",
    "cov_matrix = risk_models.sample_cov(stock_data)\n",
    "\n",
    "# Adjust expected returns based on sentiment\n",
    "sentiment_adjustment = 0.02  \n",
    "for idx, row in augmented_df.iterrows():\n",
    "    ticker = row['ticker']\n",
    "    if ticker in returns.index:\n",
    "        returns[ticker] *= (1 + sentiment_adjustment * row['sentiment_score'])\n",
    "\n",
    "print(\"\\nAdjusted Expected Returns:\")\n",
    "for ticker in returns.index:\n",
    "    print(f\"{ticker}: {returns[ticker]:.4f}\")\n",
    "\n",
    "# Optimize portfolio\n",
    "ef = EfficientFrontier(returns, cov_matrix)\n",
    "weights = ef.max_sharpe()  \n",
    "cleaned_weights = ef.clean_weights()\n",
    "\n",
    "\n",
    "print(\"\\nOptimized Portfolio Weights:\")\n",
    "for ticker, weight in cleaned_weights.items():\n",
    "    print(f\"{ticker}: {weight:.4f}\")\n",
    "\n",
    "# Calculate and display performance metrics\n",
    "expected_annual_return, annual_volatility, sharpe_ratio = ef.portfolio_performance()\n",
    "print(\"\\nPortfolio Performance Metrics:\")\n",
    "print(f\"Expected Annual Return: {expected_annual_return:.2%}\")\n",
    "print(f\"Annual Volatility: {annual_volatility:.2%}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
